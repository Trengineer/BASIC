{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b423dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be59e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import os\n",
    "\n",
    "from os.path import join as ospj\n",
    "from os.path import expanduser\n",
    "from munch import Munch as mch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import clip\n",
    "\n",
    "from ds import prepare_coco_dataloaders_extra, prepare_flickr_dataloaders, prepare_cub_dataloaders, prepare_flo_dataloaders\n",
    "import torch.distributions as dist\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "392bf199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../ProbVLM/prob2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2184926/973026707.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "from transformers import CLIPTokenizer\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\") \n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "import torch\n",
    "device = \"cuda\"\n",
    "# Path to the saved model checkpoint ---> this pth needs to be updated\n",
    "checkpoint_path = '../ProbVLM/ckpt/ProbVLM_Coco_extra_updated_best.pth'\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671f2d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "class BayesCap_MLP(nn.Module):\n",
    "    '''\n",
    "    Baseclass to create a simple MLP\n",
    "    Inputs\n",
    "        inp_dim: int, Input dimension\n",
    "        out_dim: int, Output dimension\n",
    "        hid_dim: int, hidden dimension\n",
    "        num_layers: Number of hidden layers\n",
    "        p_drop: dropout probability \n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        inp_dim, \n",
    "        out_dim,\n",
    "        hid_dim=512, \n",
    "        num_layers=1, \n",
    "        p_drop=0,\n",
    "    ):\n",
    "        super(BayesCap_MLP, self).__init__()\n",
    "        mod = []\n",
    "        for layer in range(num_layers):\n",
    "            if layer==0:\n",
    "                incoming = inp_dim\n",
    "                outgoing = hid_dim\n",
    "                mod.append(nn.Linear(incoming, outgoing))\n",
    "                mod.append(nn.ReLU())\n",
    "            elif layer==num_layers//2:\n",
    "                incoming = hid_dim\n",
    "                outgoing = hid_dim\n",
    "                mod.append(nn.Linear(incoming, outgoing))\n",
    "                mod.append(nn.ReLU())\n",
    "                mod.append(nn.Dropout(p=p_drop))\n",
    "            elif layer==num_layers-1:\n",
    "                incoming = hid_dim\n",
    "                outgoing = out_dim\n",
    "                mod.append(nn.Linear(incoming, outgoing))\n",
    "        self.mod = nn.Sequential(*mod)\n",
    "\n",
    "        self.block_mu = nn.Sequential(\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "        self.block_alpha = nn.Sequential(\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(out_dim, out_dim),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.block_beta = nn.Sequential(\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(out_dim, out_dim),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_intr = self.mod(x)\n",
    "        #print('dbg', x_intr.shape, x.shape)\n",
    "        x_intr = x_intr + x\n",
    "        x_mu = self.block_mu(x_intr)\n",
    "        x_1alpha = self.block_alpha(x_intr)\n",
    "        x_beta = self.block_beta(x_intr)\n",
    "        return x_mu, x_1alpha, x_beta\n",
    "\n",
    "class BayesCLIP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path=None,\n",
    "        device='cuda',\n",
    "    ):\n",
    "        super(BayesCLIP, self).__init__()\n",
    "        self.clip_model = load_model_p(device, model_path)\n",
    "        self.clip_model.eval()\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.img_BayesCap = BayesCap_MLP(inp_dim=1024, out_dim=1024, hid_dim=512, num_layers=3, p_drop=0.3).to(device)\n",
    "        self.txt_BayesCap = BayesCap_MLP(inp_dim=1024, out_dim=1024, hid_dim=512, num_layers=3, p_drop=0.3).to(device)\n",
    "\n",
    "    def forward(self, i_inputs, t_inputs):\n",
    "        i_features, t_features = self.clip_model(i_inputs, t_inputs)\n",
    "\n",
    "        img_mu, img_1alpha, img_beta = self.img_BayesCap(i_features)\n",
    "        txt_mu, txt_1alpha, txt_beta = self.txt_BayesCap(t_features)\n",
    "\n",
    "        return (img_mu, img_1alpha, img_beta), (txt_mu, txt_1alpha, txt_beta), (i_features, t_features)\n",
    "\n",
    "\n",
    "class BayesCap_for_CLIP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inp_dim=1024,\n",
    "        out_dim=1024,\n",
    "        hid_dim=512,\n",
    "        num_layers=3,\n",
    "        p_drop=0.1,\n",
    "    ):\n",
    "        super(BayesCap_for_CLIP, self).__init__()\n",
    "        self.img_BayesCap = BayesCap_MLP(inp_dim=inp_dim, out_dim=out_dim, hid_dim=hid_dim, num_layers=num_layers, p_drop=p_drop)\n",
    "        self.txt_BayesCap = BayesCap_MLP(inp_dim=inp_dim, out_dim=out_dim, hid_dim=hid_dim, num_layers=num_layers, p_drop=p_drop)\n",
    "\n",
    "    def forward(self, i_features, t_features):\n",
    "        \n",
    "        # print('dbg', i_features.shape, t_features.shape)\n",
    "        img_mu, img_1alpha, img_beta = self.img_BayesCap(i_features)\n",
    "        txt_mu, txt_1alpha, txt_beta = self.txt_BayesCap(t_features)\n",
    "\n",
    "        return (img_mu, img_1alpha, img_beta), (txt_mu, txt_1alpha, txt_beta)\n",
    "    \n",
    "    \n",
    "def load_data_loader(dataset, data_dir, dataloader_config):\n",
    "    prepare_loaders = {\n",
    "        'coco': prepare_coco_dataloaders_extra,\n",
    "        'flickr': prepare_flickr_dataloaders,\n",
    "        'CUB':prepare_cub_dataloaders,\n",
    "        'FLO':prepare_flo_dataloaders\n",
    "    }[dataset]\n",
    "    if dataset == 'CUB':\n",
    "        loaders = prepare_loaders(\n",
    "            dataloader_config,\n",
    "            dataset_root=data_dir,\n",
    "            caption_root=data_dir+'/text_c10',\n",
    "            vocab_path='ds/vocabs/cub_vocab.pkl')\n",
    "    elif dataset == 'FLO':\n",
    "        loaders = prepare_loaders(\n",
    "            dataloader_config,\n",
    "            dataset_root=data_dir,\n",
    "            caption_root=data_dir+'/text_c10',)\n",
    "    else:\n",
    "        loaders = prepare_loaders(\n",
    "            dataloader_config,\n",
    "            dataset_root=data_dir,\n",
    "            vocab_path='ds/vocabs/coco_vocab.pkl')\n",
    "    return loaders\n",
    "\n",
    "def load_model_p(device, model_path=None):\n",
    "    # load zero-shot CLIP model\n",
    "    model, _ = clip.load(name='RN50',\n",
    "                         device=device,\n",
    "                         loss_type='contrastive')\n",
    "    if model_path is None:\n",
    "        # Convert the dtype of parameters from float16 to float32\n",
    "        for name, param in model.named_parameters():\n",
    "            param.data = param.data.type(torch.float32)\n",
    "    else:\n",
    "        ckpt = torch.load(model_path)\n",
    "        model.load_state_dict(ckpt['state_dict'])\n",
    "        for name, param in model.named_parameters():\n",
    "            param.data = param.data.type(torch.float32)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def sample_ggd(x_mu, x_1alpha, x_beta, num_samples=100):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Fucntion needs to be modified so that we can draw a sample from the distri\n",
    "    Sample from a GGD with parameters (mu, alpha, beta).\n",
    "    \n",
    "    Args:\n",
    "        x_mu: Tensor, the location parameter (mean).\n",
    "        x_1alpha: Tensor, the scale parameter.\n",
    "        x_beta: Tensor, the shape parameter.\n",
    "        num_samples: int, number of samples to draw.\n",
    "        \n",
    "    Returns:\n",
    "        feature_vector: Tensor, derived feature vector from GGD samples.\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    # Add a small epsilon to x_1alpha to avoid zero values\n",
    "    epsilon = 1e-6\n",
    "    x_1alpha_adjusted = x_1alpha + epsilon\n",
    "\n",
    "    # Create an approximate normal distribution\n",
    "    ggd_dist = dist.Normal(x_mu, x_1alpha_adjusted)\n",
    "\n",
    "    # Sample and compute feature vector (e.g., mean of samples)\n",
    "    samples = ggd_dist.sample((num_samples,))\n",
    "\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2994c63b",
   "metadata": {},
   "source": [
    "### Feature Vectors Extraction with GGD\n",
    "\n",
    "Iterate through each batch:\n",
    "\n",
    "1. Compute the encoding of (img, txt) using CLIP: xfT, xfI\n",
    "2. Obtain parameters using BayesCap_MLP: img_mu, img_alpha, img_beta, txt_mu, txt_alpha, txt_beta\n",
    "3. Plug values into GGD to obtain feature vectors \n",
    "\n",
    "### Compute Attention\n",
    "\n",
    "Given a batch of imgs (V), and texts (T):\n",
    "\n",
    "1. Do similarity/attention scores (dot production) of image features V and text features T. att_s = (V * T')\n",
    "2. Compute the attention weights: softmax (att_s) \n",
    "\n",
    "### Save attention maps as GALS saves the att\n",
    "\n",
    "### 1. Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f5b1b6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def save_data_loaders(loaders, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(loaders, f)\n",
    "\n",
    "def load_data_loaders(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "# Usage\n",
    "dataset = 'coco'  # coco or flickr\n",
    "data_dir = ospj('../ProbVLM/Datasets/', dataset)\n",
    "dataloader_config = mch({\n",
    "    'batch_size': 64,\n",
    "    'random_erasing_prob': 0.,\n",
    "    'traindata_shuffle': True\n",
    "})\n",
    "\n",
    "filename = '../ProbVLM/Datasets/coco/data_loaders_coco_person_21.11v2.pkl'\n",
    "loaders = load_data_loaders(filename)\n",
    "\n",
    "if loaders is None:\n",
    "    loaders = load_data_loader(dataset, data_dir, dataloader_config)\n",
    "    save_data_loaders(loaders, filename)\n",
    "\n",
    "coco_train_loader, coco_valid_loader, coco_test_loader = loaders['train'], loaders['val'], loaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c464ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesCap_for_CLIP(\n",
       "  (img_BayesCap): BayesCap_MLP(\n",
       "    (mod): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Dropout(p=0.1, inplace=False)\n",
       "      (5): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "    (block_mu): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (block_alpha): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block_beta): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (txt_BayesCap): BayesCap_MLP(\n",
       "    (mod): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Dropout(p=0.1, inplace=False)\n",
       "      (5): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "    (block_mu): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (block_alpha): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block_beta): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device='cuda'\n",
    "CLIP_Net = load_model_p(device=device, model_path=None)\n",
    "ProbVLM_Net = BayesCap_for_CLIP(inp_dim=1024,\n",
    "        out_dim=1024,\n",
    "        hid_dim=512,\n",
    "        num_layers=3,\n",
    "        p_drop=0.1,\n",
    "    )\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "ProbVLM_Net = ProbVLM_Net.to(device)\n",
    "ProbVLM_Net.load_state_dict(checkpoint)\n",
    "ProbVLM_Net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1427ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22bc8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\"an image of a person\", \"a photo of a person\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c449bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grad_cam import GradCAM\n",
    "import torchray\n",
    "from torchray.attribution.grad_cam import grad_cam as tr_gradcam\n",
    "import attention_utils_p as au"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fff8f0",
   "metadata": {},
   "source": [
    "### 2. Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2a2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionVLModel(nn.Module):\n",
    "    def __init__(self, base_model, gradcam_layer='layer4.2.relu'):\n",
    "        super(AttentionVLModel, self).__init__()\n",
    "        self.base_model = base_model.module\n",
    "        self.gradcam = GradCAM(model=self.base_model, candidate_layers=[gradcam_layer])\n",
    "    \n",
    "    def forward(self, image_path, img_f, text_f, text_list, tok_t, tok_i, device):\n",
    "        \n",
    "        # Generate attention map\n",
    "        attention_data = au.clip_gcam_prob(\n",
    "            model=self.base_model,\n",
    "            file_path=image_path,\n",
    "            text_list=text_list,\n",
    "            img_f = img_f,\n",
    "            text_f = text_f,\n",
    "            tokenized_text = tok_t,\n",
    "            tokenized_img = tok_i,\n",
    "            layer=self.gradcam.candidate_layers[0],\n",
    "            device=device,\n",
    "            plot_vis=False,\n",
    "            save_vis_path = False\n",
    "        )\n",
    "        \n",
    "        # Extract relevant outputs\n",
    "        attentions, probs = attention_data['attentions'], attention_data['probs']\n",
    "        unnormalized_attentions, text_list = attention_data['unnormalized_attentions'], attention_data['text_list']\n",
    "        \n",
    "        return attentions, probs, unnormalized_attentions, text_list\n",
    "    \n",
    "attention_model = AttentionVLModel(base_model=CLIP_Net).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b410064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_text(coded_text):\n",
    "    \n",
    "    # Convert indices to tokens\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(indices.tolist()) for indices in coded_text]\n",
    "    \n",
    "    # Define a list of unwanted tokens\n",
    "    unwanted_tokens = {'<|startoftext|>', '<|endoftext|>', '.', '!', '</w>'}\n",
    "\n",
    "    # Filter the tokens to exclude unwanted ones and keep only the actual words\n",
    "    filtered_words = [token[:-4] if token.endswith('</w>') else token for token in tokens if token not in unwanted_tokens]\n",
    "\n",
    "    # Convert the list of words into a single string\n",
    "    result_string = ' '.join(filtered_words)\n",
    "    \n",
    "    return [result_string]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc71169",
   "metadata": {},
   "source": [
    "### The function that handles the saving of vis and maps as pth for both mean and median and both val or train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39169fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_attention_maps(data_loader, save_folder, method, max_files=15):\n",
    "    # Define root and save paths\n",
    "    ROOT = \"../ProbVLM/Datasets/coco/images\"\n",
    "    vis_root = os.path.join(ROOT, f\"clip_rn50_attention_gradcam_{method}\")\n",
    "    base = os.path.join(vis_root, 'data/COCO')\n",
    "    SAVE_PATH = os.path.join(base, save_folder)\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)    \n",
    "\n",
    "    print(f\"Starting to save {save_folder} with {aggregation_method} aggregation method!\")\n",
    "      \n",
    "    num_visualized = 0\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        xI, xT, paths = batch[0].to(device), batch[1].to(device), batch[4]\n",
    "        \n",
    "        for t, (img, txt, path) in enumerate(zip(xI, xT, paths)):\n",
    "            text_list = token_to_text(xT[t])\n",
    "\n",
    "            # Filter by text content\n",
    "            if text_list[0] not in {'a photo of a person', 'an image of a person'}:\n",
    "                continue\n",
    "\n",
    "            # Prepare inputs\n",
    "            img = img.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "            txt = txt.unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                xfI, xfT = CLIP_Net(img, txt)\n",
    "                #(img_mu, img_1alpha, img_beta), (txt_mu, txt_1alpha, txt_beta) = ProbVLM_Net(xfI, xfT)\n",
    "                #img_samples = sample_ggd(img_mu, img_1alpha, img_beta, 30)\n",
    "                #txt_samples = sample_ggd(txt_mu, txt_1alpha, txt_beta, 30)\n",
    "            \n",
    "#             attentions_list = []\n",
    "#             unnormalized_attentions_list = []\n",
    "#             probss = []\n",
    "\n",
    "#             for img_feature_vector, txt_feature_vector in zip(img_samples, txt_samples):\n",
    "            attentions, probs, unnorm_attentions, text_list = attention_model.forward(\n",
    "                image_path=path,\n",
    "                img_f=xfI.to(device),\n",
    "                text_f=xfT.detach(),\n",
    "                text_list=text_list,\n",
    "                tok_t=txt,\n",
    "                tok_i=img,\n",
    "                device=device\n",
    "            )\n",
    "#                 attentions_list.append(attentions)\n",
    "#                 unnormalized_attentions_list.append(unnorm_attentions)\n",
    "#                 probss.append(probs)\n",
    "\n",
    "            # Aggregate attentions\n",
    "#             if aggregation_method == 'median':\n",
    "#                 aggregated_attention = torch.median(torch.stack(attentions_list), dim=0).values\n",
    "#                 aggregated_unnorm_attention = torch.median(torch.stack(unnormalized_attentions_list), dim=0).values\n",
    "#                 aggregated_probs = np.median(probss, axis=0)\n",
    "#             elif aggregation_method == 'mean':\n",
    "#                 aggregated_attention = torch.mean(torch.stack(attentions_list), dim=0)\n",
    "#                 aggregated_unnorm_attention = torch.mean(torch.stack(unnormalized_attentions_list), dim=0)\n",
    "#                 aggregated_probs = np.mean(probss, axis=0)\n",
    "#             else:\n",
    "#                 raise ValueError(\"Aggregation method must be 'mean' or 'median'\")\n",
    "\n",
    "            # Save .pth file \n",
    "            attention_save_path = os.path.join(SAVE_PATH, f\"{os.path.basename(path).replace('.jpg', '.pth')}\")\n",
    "\n",
    "            torch.save({\n",
    "                'attentions': aggregated_attention,\n",
    "                'unnormalized_attentions': aggregated_unnorm_attention,\n",
    "                'probs': aggregated_probs,\n",
    "                'text_list': text_list\n",
    "            }, attention_save_path)\n",
    "\n",
    "            # Save visualization\n",
    "            os.makedirs(os.path.join(SAVE_PATH, 'vis'), exist_ok=True)\n",
    "            save_vis_path = os.path.join(vis_root, 'vis', os.path.basename(path).replace('.jpg', '.jpg'))\n",
    "\n",
    "            if num_visualized < max_files and i % 50 == 0:\n",
    "                au.plot_attention_helper_p(\n",
    "                    image=img,\n",
    "                    attentions=[aggregated_attention],\n",
    "                    unnormalized_attentions=[aggregated_unnorm_attention],\n",
    "                    probs=[aggregated_probs],\n",
    "                    text_list=text_list,\n",
    "                    save_vis_path=save_vis_path,\n",
    "                    resize=False\n",
    "                )\n",
    "                num_visualized += 1\n",
    "    # Done statement after processing all batches in the data_loader\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a25b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_attention_maps(coco_valid_loader, \"val2014\", \"GALS\")\n",
    "process_attention_maps(coco_train_loader, \"train2014\", \"GALS\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (prob2)",
   "language": "python",
   "name": "prob2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
